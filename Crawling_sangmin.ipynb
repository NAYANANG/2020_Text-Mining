{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import urllib.request as ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#세계일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                             \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                             stdin=PIPE)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chromedriver.exe': 'chromedriver.exe'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c59403ee2c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrowser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chromedriver.exe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.segye.com/search'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             log_path=service_log_path)\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[1;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m---> 83\u001b[0;31m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[1;32m     84\u001b[0m                 )\n\u001b[1;32m     85\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n"
     ]
    }
   ],
   "source": [
    "browser=webdriver.Chrome('chromedriver.exe')\n",
    "browser.get('https://www.segye.com/search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segye_url(keyword):\n",
    "    browser.find_element_by_id('searchWord').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"wps_layout1_box1\"]/div/div[2]/img[1]').click()\n",
    "    time.sleep(10)\n",
    "    browser.find_element_by_xpath('//*[@id=\"articleTitleArea\"]/div/a').click()\n",
    "    time.sleep(10)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"pagingNav\"]/a[2]').click()\n",
    "    time.sleep(2)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"pagingNav\"]/a[4]').click()\n",
    "    time.sleep(2)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"pagingNav\"]/a[5]').click()\n",
    "    time.sleep(2)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        html=browser.page_source\n",
    "        time.sleep(2)\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h3',{'id':'title_sns'}).text\n",
    "        source=soup.find('article',{'class':'viewBox'}).text\n",
    "        source=source.replace('\\n','').replace('\\xa0','')\n",
    "        dict[title]=source\n",
    "        \n",
    "    with open('세계일보_{}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/세계일보_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='세계일보'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('세계일보_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국경제TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korean_eco(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://search.hankyung.com/apps.frm/search.news?query={0}&sort=RANK%2FDESC%2CDATE%2FDESC&period=DATE&area=ALL&mediaid_clust=HKPAPER%2CHKCOM&sdate=2020.07.01&edate=2020.10.09&exact=&include=&except='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[1]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[4]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[5]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]/div[2]/div/span/a[6]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h1',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'id':'articletxt'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "    \n",
    "    with open('한국경제TV_{}.csv'.format(keyword),'w',-1,encoding='utf=8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/한국경제TV_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='한국경제TV'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('한국경제TV_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연합뉴스tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yonhap(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=2&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=3&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=4&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=5&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    dict={}\n",
    "\n",
    "    for i in range(len(url)):\n",
    "        browser.get('https://www.yonhapnewstv.co.kr/'+url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('strong',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'id':'articleBody'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "    with open('연합뉴스_{0}.csv'.format(keyword),'w',-1,encoding='utf=8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "    a=pd.read_csv('C:/Users/dsp12/Downloads/연합뉴스_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='연합뉴스'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('연합뉴스_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBN(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('http://search.mbn.co.kr:8080/MBN/search.jsp')\n",
    "    browser.find_element_by_xpath('//*[@id=\"query\"]').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"header\"]/div[2]/form/fieldset/div[1]/input[2]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"mainContent\"]/div[1]/ul[1]/li[4]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"mainContent\"]/div[1]/ul[2]/li[2]/input[1]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"range4\"]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"mainContent\"]/div[1]/ul[3]/li[2]/div/ul/li[6]/a/img').click()\n",
    "   \n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[4]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[5]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[6]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        time.sleep(5)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('div',{'class':'box01'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'detail'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    \n",
    "    with open('MBN뉴스_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "    a=pd.read_csv('C:/Users/user/MBN뉴스_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='MBN뉴스'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('MBN뉴스_{}.csv'.format(keyword),encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tv조선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TV_chosun(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('http://search.tvchosun.com/tvchosun/news.search?query={0}&starget=&scomplex=or&sdate=&categoryname=&categoryd2=&pageno=1&orderby=news&skind=&requery={1}&cont5=&category={2}&kind=&cont4=tab&sline=Y'.format(keyword,keyword,'뉴스'))\n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"wrap\"]/div[2]/div[3]/span/a[1]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"wrap\"]/div[2]/div[3]/span/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"wrap\"]/div[2]/div[3]/span/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "\n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h3',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'article_detail_body'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    with open('TV조선_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values()) \n",
    "        \n",
    "    a=pd.read_csv('C:/Users/dsp12/Downloads/TV조선_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='TV조선'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('TV조선_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbc(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=0'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=1'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe') \n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=2'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=3'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=4'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=5'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=6'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=7'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=8'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=9'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=10'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=11'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=12'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "\n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get('https://imnews.imbc.com'+url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h2',{'class':'art_title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'news_txt'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    with open('mbc_{}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/MBC_{}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='MBC'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('MBC_{}.csv'.format(keyword),encoding='utf-8-sig')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news1(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.news1.kr')\n",
    "    browser.find_element_by_xpath('//*[@id=\"query\"]').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"pront_top_search\"]/button').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"lnb\"]/ul/li[2]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"lnb\"]/div/fieldset[1]/ul/li[1]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"range_3M\"]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"lnb\"]/div/fieldset[2]/div/div/a').click()\n",
    "    \n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul > li > dl > dt >a ')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[3]/span/a[1]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul > li > dl > dt >a ')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[3]/span/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul > li > dl > dt >a ')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('div',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'detail sa_area'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','').replace('\\xa0','')\n",
    "        dict[title]=text\n",
    "\n",
    "    with open('news1_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/dsp12/Downloads/news1_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='news1'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('news1_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ktv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ktv(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('http://www.ktv.go.kr/')\n",
    "    browser.find_element_by_xpath('//*[@id=\"header_search_btn\"]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"header_query\"]').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"header\"]/div[2]/div/div[2]/div[2]/form/button').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[2]/ul/li[2]/a').click()\n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[4]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[5]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[6]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[7]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[8]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[9]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[10]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get('http://www.ktv.go.kr'+url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.select('div.top_view > div.lft>h2')\n",
    "        title=title[0].text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'article zoominout'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','').replace('\\xa0','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    with open('KTV국민방송_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "    \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/KTV국민방송_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='KTV국민방송'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('KTV국민방송_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
