{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import urllib.request as ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#세계일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser=webdriver.Chrome('chromedriver.exe')\n",
    "browser.get('https://www.segye.com/search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segye_url(keyword):\n",
    "    browser.find_element_by_id('searchWord').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"wps_layout1_box1\"]/div/div[2]/img[1]').click()\n",
    "    time.sleep(10)\n",
    "    browser.find_element_by_xpath('//*[@id=\"articleTitleArea\"]/div/a').click()\n",
    "    time.sleep(10)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"pagingNav\"]/a[2]').click()\n",
    "    time.sleep(2)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"pagingNav\"]/a[4]').click()\n",
    "    time.sleep(2)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"pagingNav\"]/a[5]').click()\n",
    "    time.sleep(2)\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.listBox > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        html=browser.page_source\n",
    "        time.sleep(2)\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h3',{'id':'title_sns'}).text\n",
    "        source=soup.find('article',{'class':'viewBox'}).text\n",
    "        source=source.replace('\\n','').replace('\\xa0','')\n",
    "        dict[title]=source\n",
    "        \n",
    "    with open('세계일보_{}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/세계일보_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='세계일보'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('세계일보_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국경제TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korean_eco(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://search.hankyung.com/apps.frm/search.news?query={0}&sort=RANK%2FDESC%2CDATE%2FDESC&period=DATE&area=ALL&mediaid_clust=HKPAPER%2CHKCOM&sdate=2020.07.01&edate=2020.10.09&exact=&include=&except='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[1]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[4]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[5]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]/div[2]/div/span/a[6]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h1',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'id':'articletxt'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "    \n",
    "    with open('한국경제TV_{}.csv'.format(keyword),'w',-1,encoding='utf=8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/한국경제TV_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='한국경제TV'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('한국경제TV_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연합뉴스tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yonhap(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=2&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=3&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=4&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.yonhapnewstv.co.kr/search/news?pageNo=5&q={0}&sortField=score&sort=on&startDate=&endDate='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul >li >div> div> a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    dict={}\n",
    "\n",
    "    for i in range(len(url)):\n",
    "        browser.get('https://www.yonhapnewstv.co.kr/'+url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('strong',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'id':'articleBody'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "    with open('연합뉴스_{0}.csv'.format(keyword),'w',-1,encoding='utf=8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "    a=pd.read_csv('C:/Users/dsp12/Downloads/연합뉴스_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='연합뉴스'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('연합뉴스_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBN(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('http://search.mbn.co.kr:8080/MBN/search.jsp')\n",
    "    browser.find_element_by_xpath('//*[@id=\"query\"]').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"header\"]/div[2]/form/fieldset/div[1]/input[2]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"mainContent\"]/div[1]/ul[1]/li[4]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"mainContent\"]/div[1]/ul[2]/li[2]/input[1]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"range4\"]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"mainContent\"]/div[1]/ul[3]/li[2]/div/ul/li[6]/a/img').click()\n",
    "   \n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[4]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[5]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"search_result\"]/div[3]/a[6]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.collaction_news > ul > li > a') \n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        time.sleep(5)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('div',{'class':'box01'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'detail'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    \n",
    "    with open('MBN뉴스_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "    a=pd.read_csv('C:/Users/user/MBN뉴스_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='MBN뉴스'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('MBN뉴스_{}.csv'.format(keyword),encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tv조선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TV_chosun(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('http://search.tvchosun.com/tvchosun/news.search?query={0}&starget=&scomplex=or&sdate=&categoryname=&categoryd2=&pageno=1&orderby=news&skind=&requery={1}&cont5=&category={2}&kind=&cont4=tab&sline=Y'.format(keyword,keyword,'뉴스'))\n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"wrap\"]/div[2]/div[3]/span/a[1]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"wrap\"]/div[2]/div[3]/span/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"wrap\"]/div[2]/div[3]/span/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.detail > p.article_tit > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "\n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h3',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'article_detail_body'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    with open('TV조선_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values()) \n",
    "        \n",
    "    a=pd.read_csv('C:/Users/dsp12/Downloads/TV조선_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='TV조선'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('TV조선_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbc(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=0'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=1'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe') \n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=2'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=3'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=4'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=5'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=6'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=7'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=8'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=9'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=10'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=11'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://imnews.imbc.com/more/search/?search_kwd={}#page=12'.format('코로나방역'))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.thumb_type > li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "\n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get('https://imnews.imbc.com'+url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h2',{'class':'art_title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'news_txt'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    with open('mbc_{}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/MBC_{}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='MBC'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('MBC_{}.csv'.format(keyword),encoding='utf-8-sig')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news1(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('https://www.news1.kr')\n",
    "    browser.find_element_by_xpath('//*[@id=\"query\"]').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"pront_top_search\"]/button').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"lnb\"]/ul/li[2]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"lnb\"]/div/fieldset[1]/ul/li[1]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"range_3M\"]/a').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"lnb\"]/div/fieldset[2]/div/div/a').click()\n",
    "    \n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul > li > dl > dt >a ')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[3]/span/a[1]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul > li > dl > dt >a ')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[3]/span/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul > li > dl > dt >a ')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('div',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'detail sa_area'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','').replace('\\xa0','')\n",
    "        dict[title]=text\n",
    "\n",
    "    with open('news1_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "        \n",
    "    a=pd.read_csv('C:/Users/dsp12/Downloads/news1_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='news1'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('news1_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ktv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ktv(keyword):\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    browser.get('http://www.ktv.go.kr/')\n",
    "    browser.find_element_by_xpath('//*[@id=\"header_search_btn\"]').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"header_query\"]').send_keys(keyword)\n",
    "    browser.find_element_by_xpath('//*[@id=\"header\"]/div[2]/div/div[2]/div[2]/form/button').click()\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[2]/ul/li[2]/a').click()\n",
    "    url=[]\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[4]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[5]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[6]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[7]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[8]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[9]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"contents\"]/div[6]/a[10]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('div.result-cont > ul.board-list01 >li > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "        \n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get('http://www.ktv.go.kr'+url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.select('div.top_view > div.lft>h2')\n",
    "        title=title[0].text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'class':'article zoominout'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','').replace('\\xa0','')\n",
    "        dict[title]=text\n",
    "        \n",
    "    with open('KTV국민방송_{0}.csv'.format(keyword),'w',-1,encoding='utf-8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "    \n",
    "    a=pd.read_csv('C:/Users/user/Downloads/KTV국민방송_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    a=a.transpose()\n",
    "    b=list\n",
    "    title=b(a.index)\n",
    "    c=list\n",
    "    text=a[0]\n",
    "    text=c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    source=source.iloc[:60]\n",
    "    source['Press']='KTV국민방송'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('KTV국민방송_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
