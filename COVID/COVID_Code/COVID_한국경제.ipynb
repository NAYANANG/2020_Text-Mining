{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " from selenium import webdriver\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import urllib.request as ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korean_eco(keyword):\n",
    "    #크롬 드라이버 가져오기\n",
    "    browser=webdriver.Chrome('chromedriver.exe')\n",
    "    #키워드에 따른한국경제 홈페이지 들어가기\n",
    "    browser.get('https://search.hankyung.com/apps.frm/search.news?query={0}&sort=RANK%2FDESC%2CDATE%2FDESC&period=DATE&area=ALL&mediaid_clust=HKPAPER%2CHKCOM&sdate=2020.07.01&edate=2020.10.09&exact=&include=&except='.format(keyword))\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    #페이지 내용에서 url 링크 찾기\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    #url데이터들 수집\n",
    "    url=[]\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    #다음 페이지로 전환\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[1]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[2]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[3]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[4]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[5]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div[2]/div[2]/div/span/a[6]').click()\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "    for urls in attr:\n",
    "        url.append(urls['href'])\n",
    "    #url에 들어가 있는 데이터들을 하나하나 읽어와서 기사 제목과 본문 추출 후 dictionary로 저장하기\n",
    "    dict={}\n",
    "    for i in range(len(url)):\n",
    "        browser.get(url[i])\n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        title=soup.find('h1',{'class':'title'}).text\n",
    "        title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "        text=soup.find('div',{'id':'articletxt'}).text\n",
    "        text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        dict[title]=text\n",
    "    #dictionary파일을 csv파일로 저장\n",
    "    with open('한국경제TV_{}.csv'.format(keyword),'w',-1,encoding='utf=8') as f:\n",
    "        w=csv.writer(f)\n",
    "        w.writerow(dict.keys())\n",
    "        w.writerow(dict.values())\n",
    "    #수정을 휘해 csv 파일 읽어오기    \n",
    "    korea_eco=pd.read_csv('C:/Users/user/Downloads/한국경제TV_{0}.csv'.format(keyword),encoding='utf-8')\n",
    "    korea_eco=korea_eco.transpose()\n",
    "    list_b=list\n",
    "    title=list_b(korea_eco.index)\n",
    "    list_c=list\n",
    "    text=korea_eco[0]\n",
    "    text=list_c(text)\n",
    "    source=pd.DataFrame({'Title':title,'Text':text})\n",
    "    #60개의 기사들만 뽑아낸다\n",
    "    source=source.iloc[:60]\n",
    "    #Press이름의 언론사 열을 붙힌다.\n",
    "    source['Press']='한국경제TV'\n",
    "    source=source[['Press','Title','Text']]\n",
    "    source.to_csv('한국경제TV_{0}.csv'.format(keyword),encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롬 드라이버 가져오기\n",
    "browser=webdriver.Chrome('chromedriver.exe')\n",
    "#키워드에 따른한국경제 홈페이지 들어가기\n",
    "browser.get('https://search.hankyung.com/apps.frm/search.news?query=%EC%BD%94%EB%A1%9C%EB%82%98%EB%B0%A9%EC%97%AD&sort=RANK%2FDESC%2CDATE%2FDESC&period=MONTH&area=ALL&mediaid_clust=HKPAPER%2CHKCOM&exact=&include=&except=')\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "#페이지 내용에서 url 링크 찾기\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "#url데이터들 수집\n",
    "url=[]\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "#다음 페이지로 전환\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[1]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[2]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[3]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[4]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[5]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[6]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[7]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[8]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[9]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/a[2]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[1]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[2]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])\n",
    "browser.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/span/a[3]').click()\n",
    "html=browser.page_source\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "attr=soup.select('ul.article > li >div.txt_wrap > a')\n",
    "for urls in attr:\n",
    "    url.append(urls['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "browser=webdriver.Chrome('chromedriver.exe')\n",
    "for i in range(len(url)):\n",
    "    browser.get(url[i])\n",
    "    html=browser.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    title=soup.find('h1',{'class':'title'}).text\n",
    "    title=title.replace('\\r','').replace('\\n','').replace('\\t','')\n",
    "    text=soup.find('div',{'id':'articletxt'}).text\n",
    "    text=text.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "    dict[title]=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary파일을 csv파일로 저장\n",
    "with open('COVID_한국경제.csv','w',-1,encoding='utf=8') as f:\n",
    "    w=csv.writer(f)\n",
    "    w.writerow(dict.keys())\n",
    "    w.writerow(dict.values())\n",
    "#수정을 휘해 csv 파일 읽어오기    \n",
    "korea_eco=pd.read_csv('./COVID_한국경제.csv',encoding='utf-8')\n",
    "korea_eco=korea_eco.transpose()\n",
    "list_b=list\n",
    "title=list_b(korea_eco.index)\n",
    "list_c=list\n",
    "text=korea_eco[0]\n",
    "text=list_c(text)\n",
    "source=pd.DataFrame({'Title':title,'Text':text})\n",
    "#60개의 기사들만 뽑아낸다\n",
    "source=source.iloc[:100]\n",
    "#Press이름의 언론사 열을 붙힌다.\n",
    "source['Press']='한국경제TV'\n",
    "source=source[['Press','Title','Text']]\n",
    "source.to_csv('COVID_한국경제.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
