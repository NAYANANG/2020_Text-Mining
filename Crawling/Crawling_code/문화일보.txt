#문화일보
browser=webdriver.Chrome('chromedriver.exe')
browser.get('http://mhsearch.munhwa.com/search.php?startCount=0&query={0}&mode=basic&sort=RANK&collection=news&range=A&startDate=1970.01.01&endDate=2020.10.11&searchField=ALL&reQuery=&reporter=&reQuery=&reporter=&searchField=ALL&div_code=ALL&sort=RANK&dateCheck=1&sYearView=2020&sMonView=07&sDayView=11&eYearView=2020&eMonView=10&eDayView=11'.format('코로나방역'))
url=[]
html=browser.page_source
soup=BeautifulSoup(html,'html.parser')
attr=soup.select('ul.list_thumb>li >div>a')
for urls in attr:
    url.append(urls['href'])
browser.find_element_by_xpath('//*[@id="content-wrap"]/div[1]/div[2]/div/a[2]').click()
html=browser.page_source
soup=BeautifulSoup(html,'html.parser')
attr=soup.select('ul.list_thumb>li >div>a')
for urls in attr:
    url.append(urls['href'])
browser.find_element_by_xpath('//*[@id="content-wrap"]/div[1]/div[2]/div/a[3]').click()
html=browser.page_source
soup=BeautifulSoup(html,'html.parser')
attr=soup.select('ul.list_thumb>li >div>a')
for urls in attr:
    url.append(urls['href'])
browser.find_element_by_xpath('//*[@id="content-wrap"]/div[1]/div[2]/div/a[4]').click()
html=browser.page_source
soup=BeautifulSoup(html,'html.parser')
attr=soup.select('ul.list_thumb>li >div>a')
for urls in attr:
    url.append(urls['href'])
browser.find_element_by_xpath('//*[@id="content-wrap"]/div[1]/div[2]/div/a[5]').click()
html=browser.page_source
soup=BeautifulSoup(html,'html.parser')
attr=soup.select('ul.list_thumb>li >div>a')
for urls in attr:
    url.append(urls['href'])
browser.find_element_by_xpath('//*[@id="content-wrap"]/div[1]/div[2]/div/a[6]').click()
html=browser.page_source
soup=BeautifulSoup(html,'html.parser')
attr=soup.select('ul.list_thumb>li >div>a')
for urls in attr:
    url.append(urls['href'])
browser.find_element_by_xpath('//*[@id="content-wrap"]/div[1]/div[2]/div/a[7]').click()
html=browser.page_source
soup=BeautifulSoup(html,'html.parser')
attr=soup.select('ul.list_thumb>li >div>a')
for urls in attr:
    url.append(urls['href'])

dict={}

for i in range(len(url)):
    browser.get(url[i])
    time.sleep(1)
    html=browser.page_source
    soup=BeautifulSoup(html,'html.parser')
    title=soup.find('span',{'class':'title'}).text
    title=title.replace('\r','').replace('\n','').replace('\t','')
    text=soup.find('div',{'id':'NewsAdContent'}).text
    text=text.replace('\n','').replace('\t','').replace('\r','').replace('\xa0','')
    dict[title]=text

with open('문화일보_의료정책.csv','w',-1,encoding='utf-8') as f:
    w=csv.writer(f)
    w.writerow(dict.keys())
    w.writerow(dict.values())

with open('문화일보_부동산정책.csv','w',-1,encoding='utf-8') as f:
    w=csv.writer(f)
    w.writerow(dict.keys())
    w.writerow(dict.values())

with open('문화일보_코로나방역.csv','w',-1,encoding='utf-8') as f:
    w=csv.writer(f)
    w.writerow(dict.keys())
    w.writerow(dict.values())

a=pd.read_csv('C:/Users/dsp12/Downloads/문화일보_의료정책.csv',encoding='utf-8')
a=a.transpose()
b=list
title=b(a.index)
c=list
text=a[0]
text=c(text)
source=pd.DataFrame({'Title':title,'Text':text})
source=source.iloc[:60]
source['Press']='문화일보'
source=source[['Press','Title','Text']]
source.to_csv('문화일보_의료정책.csv',encoding='utf-8-sig')

a=pd.read_csv('C:/Users/dsp12/Downloads/문화일보_부동산정책.csv',encoding='utf-8')
a=a.transpose()
b=list
title=b(a.index)
c=list
text=a[0]
text=c(text)
source=pd.DataFrame({'Title':title,'Text':text})
source=source.iloc[:60]
source['Press']='문화일보'
source=source[['Press','Title','Text']]
source.to_csv('문화일보_부동산정책.csv',encoding='utf-8-sig')

a=pd.read_csv('C:/Users/dsp12/Downloads/문화일보_부동산정책.csv',encoding='utf-8')
a=a.transpose()
b=list
title=b(a.index)
c=list
text=a[0]
text=c(text)
source=pd.DataFrame({'Title':title,'Text':text})
source=source.iloc[:60]
source['Press']='문화일보'
source=source[['Press','Title','Text']]
source.to_csv('문화일보_부동산정책.csv',encoding='utf-8-sig')